{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import h5py\n",
    "import torch\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating continuos files without missing information\n",
    "\n",
    "Dataset was reconstructed hour by hour, so for any two hours without missing events, the information was merged into a single file, and so on, if the next reconstructed file have no missing events. First, we ordered them as to make sure that files are sequential, then we make the merge. A file ending with _OK means no missing events. A file ending with _CUT means missing events, and that it was CUT just before the missing event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining start_date and end_date\n",
    "start_date = datetime.date(2020, 8, 25)\n",
    "end_date = datetime.date(2020, 10, 25)\n",
    "elapsed_days = (end_date - start_date).days + 1\n",
    "\n",
    "# loading the dataset\n",
    "data_dir = 'LOB Reconstruction/LOB Reconstructed'\n",
    "filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.hdf5']\n",
    "ordered_filenames = {((start_date + datetime.timedelta(days=i)).month, \n",
    "                     (start_date + datetime.timedelta(days=i)).day):[] for i in range(elapsed_days)}\n",
    "\n",
    "for i, day in enumerate(ordered_filenames):\n",
    "    for hour in range(24):\n",
    "        \n",
    "        date = start_date + datetime.timedelta(days=i)\n",
    "        filename_OK = 'LOB_' + str(date) +'_' + str(hour) + '_OK.hdf5'\n",
    "        filename_CUT = 'LOB_' + str(date) +'_' + str(hour) + '_CUT.hdf5'\n",
    "        \n",
    "        if filename_OK in filenames:\n",
    "            ordered_filenames[day].append(filename_OK)\n",
    "        elif filename_CUT in filenames:\n",
    "            ordered_filenames[day].append(filename_CUT)\n",
    "        else:\n",
    "            ordered_filenames[day].append('file_missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the hdf5 files and aggregating those with OK and continuous\n",
    "dataset=torch.empty(0, 73)\n",
    "dataset_split = 'Training'\n",
    "\n",
    "for day in ordered_filenames:\n",
    "    dataset=torch.empty(0, 73)\n",
    "    file_count = 0\n",
    "    \n",
    "    # Setting the prefix\n",
    "    i=0\n",
    "    prefix = ordered_filenames[day][i]\n",
    "    while prefix == 'file_missing':\n",
    "        prefix = ordered_filenames[day][i + 1]\n",
    "    prefix = prefix[4:15]\n",
    "\n",
    "    for filename in ordered_filenames[day]:\n",
    "        \n",
    "        if filename != 'file_missing':\n",
    "        \n",
    "            with h5py.File(os.path.join(data_dir, filename), \"r\") as f:\n",
    "                # Read datasets within hdf5 file and convert them to torch tensor\n",
    "                tensor_1 = torch.from_numpy(f['data'][:])\n",
    "                tensor_2 = torch.from_numpy(f['data_extend'][:])\n",
    "\n",
    "            # Concatenate along the second dimension (dim=1)\n",
    "            data = torch.cat((tensor_1, tensor_2), dim=1)\n",
    "\n",
    "            if 'OK' in filename:\n",
    "\n",
    "                # Concatenate the data to the whole dataset\n",
    "                dataset = torch.cat((dataset, data))\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Concatenate the data to the whole dataset, save and empty the dataset\n",
    "                dataset = torch.cat((dataset, data))\n",
    "                print('Count: ', file_count, 'Saved after having processed: ', filename)\n",
    "                with open(dataset_split + '/Unscaled/dataset_' + prefix + str(file_count) + '.t', 'wb') as f:\n",
    "                    torch.save(dataset, f)\n",
    "                dataset=torch.empty(0, 73)\n",
    "                file_count += 1\n",
    "                \n",
    "        else:\n",
    "            print('dataset_missing', day)\n",
    "            if dataset.numel(): # Saving the dataset if not saved before\n",
    "                print('Final_Count: ', file_count, 'Saved after having processed: ', filename)\n",
    "                with open(dataset_split + '/Unscaled/dataset_' + prefix + str(file_count) + '.t', 'wb') as f:\n",
    "                    torch.save(dataset, f)\n",
    "                dataset=torch.empty(0, 73)\n",
    "                file_count += 1\n",
    "            \n",
    "            \n",
    "    if dataset.numel(): # Saving the dataset if not saved before\n",
    "        print('Final_Count: ', file_count, 'Saved after having processed: ', filename)\n",
    "        with open(dataset_split + '/Unscaled/dataset_' + prefix + str(file_count) + '.t', 'wb') as f:\n",
    "            torch.save(dataset, f)\n",
    "        file_count += 1\n",
    "            \n",
    "\n",
    "    print('Processed day: ', day)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing (Creating Stationary Features)\n",
    "\n",
    "For each dataset split, (Training, Validation, and Test Set) stationary features were created. Please refer to the uploaded Draft Project.pdf for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = 'Training'\n",
    "processed_files = [name for name in os.listdir(dataset_split + '/Unscaled/') if os.path.splitext(name)[-1] == '.t']\n",
    "\n",
    "for filename in processed_files:\n",
    "    \n",
    "    with open(dataset_split + '/Unscaled/' + filename, 'rb') as f:\n",
    "    \n",
    "        # Reading the data and loading tensor\n",
    "        data = torch.load(f)\n",
    "        \n",
    "        # Defining price, volume\n",
    "        price = data[:, :40:2]\n",
    "        volume = data[:, 1:40:2]\n",
    "        \n",
    "        #PRICE-------------------\n",
    "        # Defining best_asks and best_bids\n",
    "        best_asks = price[:, 0].view(-1, 1).clone().detach()\n",
    "        best_bids = price[:, 1].view(-1, 1).clone().detach()\n",
    "        \n",
    "        # Computing the ask-relative price\n",
    "        price[:, ::2] = (price[:, ::2] - best_asks) / best_asks\n",
    "        # Computing the bid-relative price\n",
    "        price[:, 1::2] = (best_bids - price[:, 1::2]) / best_bids\n",
    "\n",
    "        # VOLUME-----------------\n",
    "        total_depth = volume.sum(axis=1).view(-1, 1).clone().detach()\n",
    "        volume[:] = volume / total_depth\n",
    "\n",
    "        \n",
    "        with open(dataset_split + '/Scaled/' + filename.replace('.t', '_scl') + '.t', 'wb') as f:\n",
    "            torch.save(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing mean and variance to be used when applying z-score\n",
    "\n",
    "In the following code, the mean and variance are computed from the stationary features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Mean\n",
    "scaled_files = [name for name in os.listdir('Training/Scaled/') if os.path.splitext(name)[-1] == '.t']\n",
    "\n",
    "total_length = 0\n",
    "total_sum_per_feature = 0\n",
    "\n",
    "for filename in scaled_files:\n",
    "    \n",
    "    with open('Training/Scaled/' + filename, 'rb') as f:\n",
    "        \n",
    "        # Reading the data and loading tensor\n",
    "        data = torch.load(f)\n",
    "        data = data[:, :40]\n",
    "        \n",
    "        # Computing some quantities\n",
    "        total_sum_per_feature += data.sum(dim=0)\n",
    "        \n",
    "        # Computing lenght of file\n",
    "        total_length += len(data)\n",
    "        \n",
    "# We finally compute the mean of each feature as follows\n",
    "mean = total_sum_per_feature / total_length\n",
    "\n",
    "# Saving scaler mean\n",
    "with open('Training/z_score/mean.t', 'wb') as f:\n",
    "    torch.save(mean, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the filenames on the Scaled folder (Stationary Features)\n",
    "scaled_files = [name for name in os.listdir('Training/Scaled/') if os.path.splitext(name)[-1] == '.t']\n",
    "\n",
    "# Initial values\n",
    "total_length = 0\n",
    "total_sum_difference_per_feature = 0\n",
    "\n",
    "for filename in scaled_files:\n",
    "    \n",
    "    with open('Training/Scaled/' + filename, 'rb') as f:\n",
    "        \n",
    "        # Reading the data and loading tensor\n",
    "        data = torch.load(f)\n",
    "        data = data[:, :40]\n",
    "        \n",
    "        # We compute the difference between each value and its mean feature value\n",
    "        differences = torch.pow(data - mean, 2) # Here the mean from computed in the previous cell is used\n",
    "        \n",
    "        # Computing some quantities\n",
    "        total_sum_difference_per_feature += differences.sum(dim=0)\n",
    "        \n",
    "        # Computing lenght of data\n",
    "        total_length += len(data)\n",
    "        \n",
    "# We finally compute the mean of each feature as follows\n",
    "variance = total_sum_difference_per_feature / total_length\n",
    "\n",
    "# Saving scaler variance\n",
    "with open('Training/z_score/variance.t', 'wb') as f:\n",
    "    torch.save(variance, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing features using mean and variance\n",
    "dataset_split = 'Training'\n",
    "scaled_files = [name for name in os.listdir(dataset_split + '/Scaled/') if os.path.splitext(name)[-1] == '.t']\n",
    "\n",
    "for filename in scaled_files:\n",
    "    \n",
    "    with open(dataset_split + '/Scaled/' + filename, 'rb') as f:\n",
    "    \n",
    "        # Reading the data and loading tensor\n",
    "        data = torch.load(f)\n",
    "        data = data[:, :40]\n",
    "        \n",
    "        # Normalizing and getting back data to cpu\n",
    "        data = ((data - mean) / torch.sqrt(variance))\n",
    "\n",
    "        with open(dataset_split + '/Normalized/' + filename.replace('scl.t', 'norm.t'), 'wb') as f:\n",
    "            torch.save(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
