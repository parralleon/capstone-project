{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import classification_report\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training, validation and test datasets\n",
    "validation= torch.load('Validation/Dataset/validation_dataset.t')\n",
    "test = torch.load('Test/Dataset/test_dataset.t')\n",
    "\n",
    "# Loading labels\n",
    "training_labels = torch.load('Training/Labels/training_labels.t')\n",
    "validation_labels = torch.load('Validation/Labels/validation_labels.t')\n",
    "test_labels = torch.load('Test/Labels/test_labels.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out shapes for validation and test\n",
    "print('Validation shape: ', validation.shape)\n",
    "print('Test shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight balance\n",
    "\n",
    "Since the dataset is not balanced, weights are computed and passed to the loss function so the training is not biased for the majority class, in this case, the stationary class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of occurrences per class\n",
    "number_of_labels=[]\n",
    "type_of_label = [0, 1, 2]\n",
    "\n",
    "# Selecting the labels\n",
    "labels = training_labels.clone().detach()\n",
    "\n",
    "for label in type_of_label:\n",
    "    total_labels = labels[labels == label].shape[0]\n",
    "    number_of_labels.append(total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the weights to be assigned so every class gets the same attention during training\n",
    "weights=torch.tensor(number_of_labels)\n",
    "weights= weights / weights.sum()\n",
    "print(weights)\n",
    "weights = 1/weights\n",
    "print(weights)\n",
    "weights = weights / weights.sum()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "\n",
    "We create different dataloaders for the training split and for the evaluation or test splits. Examples for the training were saved individually since the whole dataset did not fit on memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a Dataset object for the training and test datasets and labels\n",
    "class Dataset_no_train(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, labels):\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item=self.dataset[idx]\n",
    "        label=self.labels[idx].long()\n",
    "        \n",
    "        return item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_train(torch.utils.data.Dataset):\n",
    "    \n",
    "    # Characterizes a dataset for PyTorch\n",
    "    def __init__(self, labels):\n",
    "        # Initialization\n",
    "        self.labels = labels\n",
    "        self.list_IDs = range(len(labels))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the total number of samples\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generates one sample of data\n",
    "        \n",
    "        # Load data and get label\n",
    "        item = torch.load('Training/Dataset/' + str(index) + '.t')\n",
    "        label = self.labels[index].long()\n",
    "\n",
    "        return item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define then the training, validation and test dataset objects\n",
    "training_dataset = Dataset_train(training_labels)\n",
    "validation_dataset = Dataset_no_train(validation, validation_labels)\n",
    "test_dataset = Dataset_no_train(test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size = 512\n",
    "num_workers = 32\n",
    "pin_memory = True\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, \n",
    "                                                  batch_size=batch_size, \n",
    "                                                  shuffle=True, \n",
    "                                                  num_workers=num_workers)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    shuffle=False, \n",
    "                                                    num_workers=num_workers)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                              batch_size=batch_size, \n",
    "                                              shuffle=False, \n",
    "                                              num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training dataloader length: ', len(training_dataloader))\n",
    "print('Validation dataloader length: ', len(validation_dataloader))\n",
    "print('Test dataloader length: ', len(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_p=0.5):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: The number of expected features in the input x.\n",
    "            hidden_size: The number of features in the hidden state h.\n",
    "            num_layers: Number of recurrent layers. Would mean stacking two LSTMs together to form a stacked LSTM.\n",
    "            num_classes: Total number of classes to classify to: Up, Down or Stationary.\n",
    "            dropout_p: Probability of an element to be zeroed. Default: 0.5\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_type = 'Long Short-Term Memory'\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)        \n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape[batch_size, seq_len, input_size]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Propagate input through model\n",
    "        output, (h_n, c_n) = self.lstm(src)\n",
    "        h_n = h_n[-1].view(-1, self.hidden_size) # -1 was added to select the last hidden_states when num_layers > 1\n",
    "        h_n = self.dropout(h_n)\n",
    "        out = self.fc(h_n)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters\n",
    "input_size = 40\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "num_classes = 3\n",
    "dropout_p = 0.5\n",
    "\n",
    "# Creating the model\n",
    "model = LSTM_Model(input_size, hidden_size, num_layers, num_classes, dropout_p)\n",
    "\n",
    "# Defining some other training parameters, optimizer and loss function\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss_train=0.0\n",
    "    loss_validation=0.0\n",
    "    \n",
    "    for batch, labels in training_dataloader:    \n",
    "        # Forward pass\n",
    "        outputs=model(batch)\n",
    "        train_loss=loss_fn(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # We accumulate the loss\n",
    "        loss_train += train_loss.item()\n",
    "\n",
    "    for batch, labels in validation_dataloader: \n",
    "        \n",
    "        # We don't need to create a computation graph nor a backwards step for the validation set\n",
    "        with torch.no_grad():\n",
    "            outputs_validation= model(batch)\n",
    "            validation_loss=loss_fn(outputs_validation, labels)\n",
    "            assert validation_loss.requires_grad == False\n",
    "            \n",
    "        # We accumulate the test loss\n",
    "        loss_validation += validation_loss.item()\n",
    "            \n",
    "    if epoch == 1 or epoch % 1 == 0:\n",
    "        print('{} Epoch {}, Training loss {}, Validation loss {}'.format(datetime.datetime.now(), \n",
    "                                                                         epoch,\n",
    "                                                                         loss_train / len(dataloader_train),\n",
    "                                                                         loss_validation / len(dataloader_validation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for evaluating the \n",
    "def evaluate(model: nn.Module, dataloader: DataLoader, print_report: bool):\n",
    "    model.eval()  # Turn on evaluation mode\n",
    "    all_outputs = torch.empty(0)\n",
    "    all_labels = torch.empty(0)\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    with torch.no_grad(): # Context-manager that disabled gradient calculation\n",
    "        for batch, labels in dataloader:\n",
    "            \n",
    "            # Computing model output and loss\n",
    "            outputs= model(batch)\n",
    "            loss=loss_fn(outputs, labels)    \n",
    "            assert loss.requires_grad == False\n",
    "            \n",
    "            # Concatenating the outputs and labels for the f1-score\n",
    "            all_outputs = torch.cat((all_outputs, torch.argmax(outputs, axis=1)))\n",
    "            all_labels = torch.cat((all_labels, labels))\n",
    "            \n",
    "            # Adding the loss for each batch\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    total_loss_avg = total_loss / len(validation_dataloader) # Might be slightly different incorrect if last batch is considerably smaller than the batch_size\n",
    "    report = classification_report(all_labels.cpu(), all_outputs.cpu(), output_dict=not(print_report), digits=4)\n",
    "            \n",
    "    return total_loss_avg, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model on test\n",
    "loss_test, report_test = evaluate(model, test_dataloader, print_report=True)\n",
    "print(report_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
